{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V_Ob3PlNWzRo",
        "outputId": "21986636-7efe-457a-d6c2-b2cb483fdbc2"
      },
      "source": [
        "!pip uninstall -y clip\n",
        "!pip uninstall -y openai-clip\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: clip 1.0\n",
            "Uninstalling clip-1.0:\n",
            "  Successfully uninstalled clip-1.0\n",
            "\u001b[33mWARNING: Skipping openai-clip as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-9mzbu5cn\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-9mzbu5cn\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (6.3.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (25.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.24.0+cu126)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->clip==1.0) (0.2.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=f518667fba4b45fdcf6e5bd300f7cf54d38aba4a4f1a8d12477c49421e263079\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-t5oiyovi/wheels/35/3e/df/3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "clip"
                ]
              },
              "id": "4bb8f5f3fe95430190eec06ab5828734"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import os\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import subprocess\n",
        "import sys\n",
        "import clip"
      ],
      "metadata": {
        "id": "XGAQxTHuUsar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mount_google_drive():\n",
        "    \"\"\"Mount Google Drive to access files\"\"\"\n",
        "    try:\n",
        "        drive.mount('/content/drive', force_remount=False)\n",
        "        print(\"Google Drive mounted successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "\n",
        "# Call this at the start\n",
        "mount_google_drive()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbW1ZclqGo0r",
        "outputId": "a50b4e10-d7f6-4bf4-b880-fdbdfb4b22cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CVAE(nn.Module):\n",
        "    \"\"\"Conditional VAE - matching your teammate's architecture\"\"\"\n",
        "    def __init__(self, latent_dim=16, label_emb_dim=16):\n",
        "        super().__init__()\n",
        "\n",
        "        self.label_emb = nn.Embedding(2, label_emb_dim)\n",
        "\n",
        "        # Encoder CNN\n",
        "        self.enc_conv1 = nn.Conv2d(1, 32, 4, 2, 1)\n",
        "        self.enc_conv2 = nn.Conv2d(32, 64, 4, 2, 1)\n",
        "        self.enc_conv3 = nn.Conv2d(64, 128, 4, 2, 1)\n",
        "        self.enc_conv4 = nn.Conv2d(128, 256, 4, 2, 1)\n",
        "        self.enc_conv5 = nn.Conv2d(256, 512, 4, 2, 1)\n",
        "\n",
        "        self.fc_enc = nn.Linear(512*8*8 + label_emb_dim, 512)\n",
        "        self.fc_mu = nn.Linear(512, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(512, latent_dim)\n",
        "\n",
        "        self.fc_dec1 = nn.Linear(latent_dim + label_emb_dim, 512)\n",
        "        self.fc_dec2 = nn.Linear(512, 512*8*8)\n",
        "\n",
        "        self.dec_deconv1 = nn.ConvTranspose2d(512, 256, 4, 2, 1)\n",
        "        self.dec_deconv2 = nn.ConvTranspose2d(256, 128, 4, 2, 1)\n",
        "        self.dec_deconv3 = nn.ConvTranspose2d(128, 64, 4, 2, 1)\n",
        "        self.dec_deconv4 = nn.ConvTranspose2d(64, 32, 4, 2, 1)\n",
        "        self.dec_out = nn.ConvTranspose2d(32, 1, 4, 2, 1)\n",
        "\n",
        "    def encode(self, x, y):\n",
        "        h = F.relu(self.enc_conv1(x))\n",
        "        h = F.relu(self.enc_conv2(h))\n",
        "        h = F.relu(self.enc_conv3(h))\n",
        "        h = F.relu(self.enc_conv4(h))\n",
        "        h = F.relu(self.enc_conv5(h))\n",
        "\n",
        "        h = h.view(h.size(0), -1)\n",
        "        h = torch.cat([h, self.label_emb(y)], dim=1)\n",
        "        h = F.relu(self.fc_enc(h))\n",
        "\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = torch.clamp(self.fc_logvar(h), -8, 8)\n",
        "        return mu, logvar\n",
        "\n",
        "    def decode(self, z, y):\n",
        "        h = torch.cat([z, self.label_emb(y)], dim=1)\n",
        "        h = F.relu(self.fc_dec1(h))\n",
        "        h = F.relu(self.fc_dec2(h))\n",
        "        h = h.view(h.size(0), 512, 8, 8)\n",
        "\n",
        "        h = F.relu(self.dec_deconv1(h))\n",
        "        h = F.relu(self.dec_deconv2(h))\n",
        "        h = F.relu(self.dec_deconv3(h))\n",
        "        h = F.relu(self.dec_deconv4(h))\n",
        "\n",
        "        return torch.sigmoid(self.dec_out(h))\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        mu, logvar = self.encode(x, y)\n",
        "        z = mu + torch.randn_like(mu) * torch.exp(0.5 * logvar)\n",
        "        recon = self.decode(z, y)\n",
        "        return recon, mu, logvar\n",
        "\n",
        "\n",
        "class CVAELoader:\n",
        "    def __init__(self, model_path, device='cuda'):\n",
        "        self.device = device\n",
        "        self.model = self._load_cvae('/content/drive/MyDrive/solar/model_b1_2')\n",
        "\n",
        "    def _load_cvae(self, model_path):\n",
        "        \"\"\"Load pre-trained CVAE model\"\"\"\n",
        "        model_path = Path(model_path)\n",
        "\n",
        "        if not model_path.exists():\n",
        "            raise FileNotFoundError(f\"Path does not exist: {model_path}\")\n",
        "\n",
        "        pt_files = [f for f in model_path.iterdir() if f.suffix == '.pt']\n",
        "\n",
        "        if not pt_files:\n",
        "            raise FileNotFoundError(f\"No .pt files found in {model_path}\")\n",
        "\n",
        "        pt_file = pt_files[0]\n",
        "        print(f\"Loading CVAE from {pt_file}\")\n",
        "\n",
        "        model = CVAE(latent_dim=2, label_emb_dim=16).to(self.device)\n",
        "\n",
        "        # Load checkpoint\n",
        "        checkpoint = torch.load(str(pt_file), map_location=self.device)\n",
        "\n",
        "        if isinstance(checkpoint, dict):\n",
        "            if 'model' in checkpoint:\n",
        "                state_dict = checkpoint['model']\n",
        "            elif 'state_dict' in checkpoint:\n",
        "                state_dict = checkpoint['state_dict']\n",
        "            else:\n",
        "                state_dict = checkpoint\n",
        "        else:\n",
        "            state_dict = checkpoint\n",
        "\n",
        "        try:\n",
        "            model.load_state_dict(state_dict)\n",
        "            print(\"✓ Model loaded with strict=True\")\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Warning: {e}\")\n",
        "            print(\"Loading with strict=False\")\n",
        "            model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "        model.eval()\n",
        "        return model\n",
        "\n",
        "    def decode(self, z, label):\n",
        "        \"\"\"Decode latent to image for a given label\"\"\"\n",
        "        with torch.no_grad():\n",
        "            y = torch.full((z.shape[0],), label, dtype=torch.long, device=self.device)\n",
        "            return self.model.decode(z, y)"
      ],
      "metadata": {
        "id": "ubuQ7T_QU8bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SolarFlareDataset(Dataset):\n",
        "    def __init__(self, csv_path, image_dir, image_size=224):\n",
        "        self.df = pd.read_csv('/content/drive/MyDrive/solar/labels_with_captions.csv')\n",
        "        self.image_dir = Path('/content/drive/MyDrive/solar/preprocessed_hourly_all')\n",
        "        self.image_size = image_size\n",
        "        self.valid_indices = []\n",
        "\n",
        "        for idx, row in self.df.iterrows():\n",
        "            try:\n",
        "                image_filename = row.get('SHARP_FILE', None) or row.iloc[1]\n",
        "                image_path = self.image_dir / str(image_filename)\n",
        "                if image_path.exists():\n",
        "                    self.valid_indices.append(idx)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        print(f\"Found {len(self.valid_indices)} valid images out of {len(self.df)}\")\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((image_size, image_size)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                std=[0.26862954, 0.26130258, 0.27577711]\n",
        "            )\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_indices)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        actual_idx = self.valid_indices[idx]\n",
        "        row = self.df.iloc[actual_idx]\n",
        "\n",
        "        caption = str(row.get('caption', row.iloc[0]))\n",
        "\n",
        "        if pd.isna(caption) or caption == 'nan':\n",
        "            caption = \"A solar flare image\"\n",
        "\n",
        "        image_filename = row.get('Image filename', row.iloc[1])\n",
        "        image_path = self.image_dir / str(image_filename)\n",
        "\n",
        "        try:\n",
        "            img = Image.open(image_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {image_path}: {e}\")\n",
        "            img = Image.new('RGB', (self.image_size, self.image_size))\n",
        "\n",
        "        img_tensor = self.transform(img)\n",
        "\n",
        "        return {\n",
        "            'image': img_tensor,\n",
        "            'caption': caption,\n",
        "            'flare_label': row.get('Flare label', 0)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "u_khIH5pFKFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import clip\n",
        "import numpy as np\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "from PIL import Image, ImageEnhance\n",
        "\n",
        "class CLIPGuidedGenerator:\n",
        "    def __init__(self, cvae_model, device='cuda'):\n",
        "        self.device = device\n",
        "        self.cvae = cvae_model\n",
        "        self.latent_dim = cvae_model.fc_mu.out_features\n",
        "        print(f\"CVAE latent dimension detected: {self.latent_dim}\")\n",
        "\n",
        "        print(\"Loading pre-trained CLIP model (ViT-B/32)...\")\n",
        "        self.clip_model, self.clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "        self.clip_model.eval()\n",
        "\n",
        "    def generate(self, text_prompt, label=1, steps=300, lr=0.1, clip_weight=1.0):\n",
        "        \"\"\"Generate solar flare image from text prompt using CLIP guidance.\"\"\"\n",
        "\n",
        "        label_tensor = torch.tensor([label], dtype=torch.long, device=self.device)\n",
        "\n",
        "        z = torch.randn(1, self.latent_dim, device=self.device, requires_grad=True)\n",
        "        optimizer = torch.optim.Adam([z], lr=lr)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            text_tokens = clip.tokenize([text_prompt]).to(self.device)\n",
        "            text_features = self.clip_model.encode_text(text_tokens)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        best_loss = float('inf')\n",
        "        best_z = z.clone().detach()\n",
        "\n",
        "        print(f\"Generating image for prompt: '{text_prompt}'\")\n",
        "        print(f\"Label: {'FL (Flare)' if label==1 else 'NF (Non-Flare)'}\")\n",
        "\n",
        "        for step in range(steps):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            img = self.cvae.decode(z, label_tensor)\n",
        "\n",
        "            if img.shape[1] == 1:\n",
        "                img_rgb = img.repeat(1, 3, 1, 1)\n",
        "            else:\n",
        "                img_rgb = img\n",
        "\n",
        "            img_rgb = F.interpolate(img_rgb, size=(224,224), mode='bilinear', align_corners=False)\n",
        "\n",
        "\n",
        "            image_features = self.clip_model.encode_image(img_rgb)\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            clip_loss = -clip_weight * torch.nn.functional.cosine_similarity(text_features, image_features).mean()\n",
        "            reg_loss = 1e-4 * z.abs().mean()\n",
        "            loss = clip_loss + reg_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if loss.item() < best_loss:\n",
        "                best_loss = loss.item()\n",
        "                best_z = z.clone().detach()\n",
        "\n",
        "            if (step + 1) % 50 == 0:\n",
        "                print(f\"Step {step+1}/{steps}, Loss: {loss.item():.6f}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            final_img = self.cvae.decode(best_z, label_tensor)\n",
        "\n",
        "        return final_img\n",
        "\n",
        "    def save_image(self, tensor, save_path):\n",
        "        save_path = Path(save_path)\n",
        "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        img = tensor.squeeze().detach().cpu().numpy()\n",
        "        if len(img.shape) == 2:\n",
        "            img = np.stack([img]*3, axis=0)\n",
        "        elif img.shape[0] == 1:\n",
        "            img = np.repeat(img, 3, axis=0)\n",
        "\n",
        "        img = np.transpose(img, (1,2,0))\n",
        "        img = (img * 255).clip(0,255).astype(np.uint8)\n",
        "\n",
        "\n",
        "        pil_img = Image.fromarray(img)\n",
        "\n",
        "\n",
        "        pil_img = ImageEnhance.Contrast(pil_img).enhance(2.0)\n",
        "        pil_img = ImageEnhance.Color(pil_img).enhance(1.5)\n",
        "\n",
        "        pil_img.save(save_path)\n",
        "        print(f\"✓ Image saved with enhancement to {save_path}\")"
      ],
      "metadata": {
        "id": "yBrpJB-HHn9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    cvae_model_path = \"/content/drive/MyDrive/solar/model_b1_2\"\n",
        "    output_dir = Path(\"/content/drive/MyDrive/solar/clip\")\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        cvae_loader = CVAELoader(cvae_model_path, device=device)\n",
        "        cvae = cvae_loader.model\n",
        "        print(\"✓ CVAE model loaded successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading CVAE: {e}\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    generator = CLIPGuidedGenerator(cvae, device=device)\n",
        "    print(\"✓ Generator initialized\")\n",
        "\n",
        "    prompts = [\n",
        "    'Nan solar flare on 2010-05-01. Active region longitudes -82 to -74. Magnetic flux ratio 0.0'\n",
        "]\n",
        "\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        print(f\"\\n[{i+1}/{len(prompts)}] Generating image...\")\n",
        "        try:\n",
        "            img = generator.generate(prompt, label=1, steps=300, lr=0.1, clip_weight=1.0)\n",
        "            output_path = output_dir / f\"testgenerated_flare_{i+1}.png\"\n",
        "            generator.save_image(img, output_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating image: {e}\")\n",
        "\n",
        "    print(\"\\nGeneration complete! Images saved to Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VyICvetK_Lb",
        "outputId": "4c4981eb-b5c5-48a1-d28f-2ae3f89e4fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loading CVAE from /content/drive/MyDrive/solar/model_b1_2/model_b1_2.pt\n",
            "✓ Model loaded with strict=True\n",
            "✓ CVAE model loaded successfully\n",
            "CVAE latent dimension detected: 2\n",
            "Loading pre-trained CLIP model (ViT-B/32)...\n",
            "✓ Generator initialized\n",
            "\n",
            "[1/1] Generating image...\n",
            "Generating image for prompt: 'Nan solar flare on 2010-05-01. Active region longitudes -82 to -74. Magnetic flux ratio 0.0'\n",
            "Label: FL (Flare)\n",
            "Step 50/300, Loss: -0.181757\n",
            "Step 100/300, Loss: -0.181757\n",
            "Step 150/300, Loss: -0.181758\n",
            "Step 200/300, Loss: -0.181758\n",
            "Step 250/300, Loss: -0.181757\n",
            "Step 300/300, Loss: -0.181757\n",
            "✓ Image saved with enhancement to /content/drive/MyDrive/solar/clip/testgenerated_flare_1.png\n",
            "\n",
            "Generation complete! Images saved to Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y5YqivtfW9p3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}